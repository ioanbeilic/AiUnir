# üìö RESUMEN SEMANA 3 - M√≥dulo 1, Temas 2-3

**Periodo**: 24-28 Noviembre 2025
**Material**: Secciones 3-4 (p√°ginas 61-120)
**Temas**: Utilizaci√≥n de Modelos de IA y Procesamiento del Lenguaje Natural

---

## TEMA 2: UTILIZACI√ìN DE MODELOS DE INTELIGENCIA ARTIFICIAL

### 2.1 ¬øPor qu√© es importante la IA?

**Tres clases principales de sistemas empresariales**:
- Sistemas informacionales
- Sistemas transaccionales
- Sistemas inteligentes (enfoque del m√°ster)

**Aportes concretos de la IA**:

1. **Automatiza el aprendizaje por repetici√≥n**:
   - Realiza tareas frecuentes con alto volumen de datos
   - Totalmente fiable y sin fatiga
   - Llamada "automatizaci√≥n inteligente" (Intelligent Automation)

2. **Agrega inteligencia a productos existentes**:
   - Mejora productos ya desarrollados
   - Ejemplo: Siri en productos Apple
   - Se combina con Big Data

3. **Se adapta mediante algoritmos de aprendizaje progresivo**:
   - Los datos realizan la programaci√≥n
   - El algoritmo se convierte en predictor o clasificador
   - Mismo algoritmo para funciones variadas

4. **Analiza datos complejos con redes neuronales profundas (Deep Learning)**:
   - Construye sistemas de detecci√≥n de fraude
   - Necesita muchos datos para entrenar
   - IoT aporta datos donde faltan

5. **Es incre√≠blemente precisa con Deep Learning**:
   - Ejemplo: detecci√≥n de c√°ncer en resonancias magn√©ticas con precisi√≥n similar a radi√≥logos expertos

6. **Saca el mayor provecho de los datos**:
   - Los datos se vuelven propiedad intelectual
   - En un mundo "data driven", proporciona ventaja competitiva
   - Mejores datos = mejor posici√≥n

### 2.3 Modelos de Sistemas de IA

**Proceso de definici√≥n de problemas**:
1. Definir representaci√≥n de estados
2. Determinar estado inicial
3. Determinar estado final/meta
4. Definir operadores/reglas de transici√≥n

**Objetivo**: Encontrar secuencia de reglas que lleven del estado inicial al estado meta.

**Criterios de eficiencia**:
- ¬øPermite llegar a una soluci√≥n?
- ¬øEs una buena soluci√≥n?
- Coste de b√∫squeda (tiempo y memoria)
- Coste total (b√∫squeda + camino)

### Automatizaci√≥n de Tareas

**Beneficios inmediatos**:
- Reducir costos generales
- Aumentar productividad
- Aumentar disponibilidad
- Mejorar precisi√≥n y fiabilidad
- Mejorar rendimiento en todos los departamentos
- Mejorar enfoque del problema

**Concepto de Hiperautomatizaci√≥n**: Trasladar tareas rutinarias de personas hacia sistemas autom√°ticos.

**Las 4 Capacidades de la Automatizaci√≥n Inteligente** (seg√∫n Bornet, Barkin y Wirtz):

#### 1. Capacidad de Visi√≥n
- Percepci√≥n, interpretaci√≥n y comprensi√≥n del mundo visual
- **Tecnolog√≠as clave**:
  - OCR (Reconocimiento √ìptico de Caracteres)
  - ICR (Reconocimiento Inteligente de Caracteres)
  - An√°lisis de im√°genes y v√≠deos
  - Biometr√≠a avanzada
- **Aplicaciones**: Diagn√≥stico m√©dico por imagen, identificaci√≥n de delincuentes

#### 2. Capacidad de Ejecuci√≥n
- "Hacer" tareas en entornos digitales
- Completar formularios, escribir texto, enrutar informaci√≥n
- **Tecnolog√≠as clave**:
  - Workflow inteligente
  - Plataformas Low Code
  - RPA (Robot Process Automation)
- **Funci√≥n**: Une las otras tres capacidades

#### 3. Capacidad de Lenguaje
- Leer, hablar, escribir, interactuar con lenguaje humano
- **Aplicaciones**:
  - Traducci√≥n de texto
  - Extracci√≥n de informaci√≥n
  - An√°lisis de sentimientos
  - Voz a texto / texto a voz
  - Clasificaci√≥n de informaci√≥n
- **Tecnolog√≠a**: Procesamiento del Lenguaje Natural (PNL)

#### 4. Capacidad de Pensar y Aprender
- Relacionada con toma de decisiones
- Crea conocimiento mediante an√°lisis y predicci√≥n
- **Tecnolog√≠as clave**:
  - Big Data
  - Machine Learning
  - Visualizaci√≥n de datos
- **Funci√≥n**: Proporciona informaci√≥n para decisiones

### Sistemas Basados en Reglas

**Componentes**:
- **Base de conocimiento**: Datos (hechos) + Conocimiento (reglas)
- **Motor de inferencia**: Manipula componentes y extrae conclusiones

**Estructura de regla**: "Si premisa, entonces conclusi√≥n"

**M√©todos de inferencia**:
- **Modus Ponens**: Encadenamiento hacia adelante (de hechos a conclusiones)
- **Modus Tollens**: Encadenamiento hacia atr√°s (de conclusi√≥n a hechos)

**Ventajas**:
1. Representaci√≥n natural del conocimiento experto
2. Estructura uniforme (Si... entonces...)
3. Separaci√≥n entre base de conocimiento y procesamiento
4. Capacidad para trabajar con incertidumbre

**Desventajas**:
1. Relaciones opacas entre reglas
2. Estrategias de b√∫squeda ineficientes
3. Incapaz de aprender sin aditivos

### 2.4 Sistemas de Razonamiento Impreciso

**Factores de Certeza (CF)**:
- Valor entre -1 y +1
- +1 = certeza total
- -1 = falsedad total
- 0 = desconocido

**Propagaci√≥n con un antecedente**:
```
cf(H,E) = cf(E) ¬∑ cf(regla)
```

**Propagaci√≥n con m√∫ltiples antecedentes**:
- **AND**: Tomar el m√≠nimo de los cf
- **OR**: Tomar el m√°ximo de los cf

**Razonamiento Bayesiano**:

Teorema de Bayes:
```
P(H|E) = P(E|H) ¬∑ P(H) / P(E)
```

**Na√Øve Bayes**:
- Clasificadores probabil√≠sticos
- Suposici√≥n: atributos de entrada independientes entre s√≠ respecto a la clase
- Buenos resultados con conjuntos de entrenamiento medio/grandes

### L√≥gica Difusa (Fuzzy Logic)

**Concepto**: Extiende l√≥gica tradicional permitiendo "grado de pertenencia" a un conjunto (valores entre 0 y 1).

**Componentes**:
- **Conjunto difuso**: Distribuci√≥n de posibilidades de pertenencia
- **Funci√≥n de pertenencia (Œº)**: Define grado de pertenencia (0 a 1)
- **Variable ling√º√≠stica**: Variable cuyos valores son t√©rminos ling√º√≠sticos (bajo, medio, alto)

**Variables Ling√º√≠sticas** (X, T(X), U, G, M):
- X: nombre de la variable
- T(x): conjunto de valores ling√º√≠sticos
- U: universo de discurso (rango de valores)
- G: regla sint√°ctica para generar valores
- M: regla sem√°ntica (asocia t√©rminos con conjuntos difusos)

**Modificadores Ling√º√≠sticos**:
- "Muy": Œº¬≤(x) - Reduce grado de pertenencia
- "M√°s o menos": ‚àöŒº(x) - Aumenta grado de pertenencia

**Reglas Difusas**:
```
SI x es a
ENTONCES y es b
```

**Ventajas**:
- Maneja imprecisi√≥n e incertidumbre
- Expresi√≥n mediante palabras de uso cotidiano
- Reduce cantidad de c√≥digo
- Controla procesos gobernados por reglas intuitivas

**Aplicaciones**: Control de procesos, medicina, biolog√≠a, econom√≠a, pol√≠tica

---

## TEMA 3: PROCESAMIENTO DEL LENGUAJE NATURAL (PLN)

### 3.1 ¬øQu√© contempla el PLN?

**Definici√≥n**: Campo interdisciplinario de la IA que tiene como objetivo que las m√°quinas realicen tareas que involucren el lenguaje humano.

**Campos involucrados**:
- Inform√°tica (an√°lisis sint√°ctico, sem√°ntica)
- Ingenier√≠a electr√≥nica/telecomunicaciones (reconocimiento de voz)
- Ling√º√≠stica computacional (morfolog√≠a, fonolog√≠a, pragm√°tica)
- Psicolog√≠a (mecanismos cognitivos)
- Sociolog√≠a (influencia social en el lenguaje)

**Nombres alternativos**:
- Procesamiento del lenguaje y del habla
- Tecnolog√≠a del lenguaje
- Ling√º√≠stica computacional
- Reconocimiento y s√≠ntesis del habla

### 3.2 Historia del PLN

#### Paradigmas Fundacionales (1940-1950)

**1. Aut√≥matas**:
- Origen: Trabajo de Turing (1936)
- Contribuciones:
  - Modelo simplificado de neurona (McCulloch y Pitts, 1943)
  - Aut√≥matas finitos y expresiones regulares (Kleene, 1951)
  - Cadenas de Markov aplicadas al lenguaje (Shannon, 1948)
  - Gram√°ticas de estados finitos (Chomsky, 1956)
- **Resultado**: Teor√≠a del lenguaje formal

**2. Teor√≠a de la Informaci√≥n**:
- Shannon: Teorema de codificaci√≥n de canal
- Concepto de entrop√≠a aplicado al lenguaje
- Primera medida de entrop√≠a del ingl√©s
- 1952: Primer sistema de reconocimiento de voz (Bell Labs)
  - 97-99% precisi√≥n para 10 d√≠gitos

#### Paradigma Simb√≥lico y Estoc√°stico (1957-1970)

**Paradigma Simb√≥lico**:
- L√≠nea 1: Teor√≠a del lenguaje formal (Chomsky)
  - Gram√°ticas libres de contexto (1956)
  - Algoritmos de an√°lisis (top-down, bottom-up)
  - TDAP: Primer sistema de an√°lisis (1958-1959)

- L√≠nea 2: Inteligencia Artificial
  - Simposio fundacional (1956): McCarthy, Minsky, Shannon, Rochester
  - Logic Theorist y General Problem Solver (Newell y Simon)
  - Primeros sistemas de comprensi√≥n del lenguaje natural
  - SHRDLU (Winograd, 1972): robot que mueve bloques

**Paradigma Estoc√°stico**:
- M√©todo bayesiano en reconocimiento √≥ptico de caracteres
- Sistema de reconocimiento de texto (Bledsoe y Browning)
- **Brown Corpus** (1963): primer corpus online
  - 1 mill√≥n de palabras
  - 500 textos de diferentes g√©neros

#### Cuatro Paradigmas de Investigaci√≥n (1970-1983)

1. **Estoc√°stico**: HMM para reconocimiento de voz
2. **Basado en L√≥gica**:
   - Q-system (Colmerauer, 1978)
   - DCG (Definite Clause Grammar)
   - LFG (Lexical Functional Grammar)
3. **Comprensi√≥n del lenguaje natural**:
   - SHRDLU
   - Escuela de Yale (Schank)
   - Redes sem√°nticas
   - Sistema LUNAR (Woods, 1967)
4. **Modelado del discurso**:
   - Estructura y enfoque del discurso (Grosz)
   - Resoluci√≥n de referencias (Hobbs)
   - Modelo BDI (creencias-deseos-intenciones)

#### Revivir del Empirismo (1983-1993)

**Modelos de Estados Finitos**:
- Fonolog√≠a y morfolog√≠a (Kaplan y Kay, 1981)
- Sintaxis (Church, 1980)

**Retorno del Empirismo**:
- Modelos probabil√≠sticos en reconocimiento de voz (IBM Watson Research)
- Etiquetado morfosint√°ctico (POS tagging)
- An√°lisis y resoluci√≥n de ambig√ºedades
- **Enfoque de evaluaci√≥n**: M√©tricas cuantitativas y comparaci√≥n de resultados
- Desarrollo en generaci√≥n de lenguaje natural

#### Uni√≥n de Vertientes (1994-1999)

**Cambios principales**:
1. Modelos probabil√≠sticos se vuelven est√°ndar
2. Explotaci√≥n comercial:
   - Reconocimiento de voz
   - Revisi√≥n de ortograf√≠a y gram√°tica
3. Comunicaci√≥n aumentativa para discapacitados
4. Auge de la web: necesidad de recuperaci√≥n y extracci√≥n de informaci√≥n

#### Auge del Aprendizaje Autom√°tico (2000-presente)

**Recursos clave**:
- **Linguistic Data Consortium (LDC)**: materiales anotados
- **Penn Treebank**: anotaciones sint√°cticas
- **Prague Dependency Treebank**: estructura de dependencias
- **PropBank**: anotaciones sem√°nticas

**Enfoques**:
- **Supervisado** (hasta 2005):
  - SVM (M√°quinas de Vectores de Soporte)
  - M√°xima entrop√≠a
  - Regresi√≥n log√≠stica multinomial
  - Modelos bayesianos

- **No supervisado** (desde 2005):
  - Traducci√≥n autom√°tica sin datos anotados
  - Modelado de temas
  - Etiquetado morfosint√°ctico (Goldwater y Griffiths, 2007)
  - Etiquetado sem√°ntico (Titov y Klementiev, 2012)

- **Deep Learning** (desde 2006):
  - T√©rmino acu√±ado por Geoffrey Hinton
  - Redes neuronales recurrentes (RNN)
  - Alternativa a HMM en an√°lisis
  - Modelos seq2seq para chatbots
  - Base de agentes conversacionales actuales

### 3.3 Conocimiento del Lenguaje en PLN

**6 Tipos de Conocimiento Necesarios**:

#### 1. Fon√©tica y Fonolog√≠a
- **Qu√© es**: Conocimiento sobre sonidos ling√º√≠sticos
- **Para qu√©**: Reconocimiento y s√≠ntesis de voz
- **C√≥mo**: Pronunciaci√≥n de palabras y generaci√≥n ac√∫stica de sonidos

#### 2. Morfolog√≠a
- **Qu√© es**: Componentes significativos de palabras
- **Para qu√©**: Reconocer variaciones (plurales, conjugaciones)
- **C√≥mo**: Descomposici√≥n en ra√≠z y terminaciones

#### 3. Sintaxis
- **Qu√© es**: Relaciones estructurales entre palabras
- **Para qu√©**: Ordenar y agrupar palabras correctamente
- **C√≥mo**: Conocimiento de estructura gramatical

#### 4. Sem√°ntica
- **Tipos**:
  - **Sem√°ntica l√©xica**: Significado de palabras individuales
  - **Sem√°ntica composicional**: Significado de combinaciones de palabras
- **Para qu√©**: Comprender el significado y la relaci√≥n con la estructura sint√°ctica

#### 5. Pragm√°tica
- **Qu√© es**: Uso del lenguaje en contexto
- **Para qu√©**: Interpretar intenciones y actos de habla

#### 6. Discurso
- **Qu√© es**: An√°lisis de texto m√°s all√° de la oraci√≥n
- **Para qu√©**: Coherencia, cohesi√≥n y referencia en el texto

**Interrelaci√≥n**: Estos conocimientos se utilizan de forma conjunta. La sintaxis y la sem√°ntica, por ejemplo, trabajan juntas en el procesamiento.

---

## üéØ CONCEPTOS CLAVE PARA MEMORIZAR

### Automatizaci√≥n Inteligente:
- **4 capacidades**: Visi√≥n, Ejecuci√≥n, Lenguaje, Pensar y Aprender
- **Hiperautomatizaci√≥n**: Trasladar tareas rutinarias humanas a sistemas autom√°ticos
- **RPA**: Robot Process Automation para ejecuci√≥n de tareas

### Sistemas de Razonamiento:
- **Factores de Certeza**: Escala de -1 a +1
- **Teorema de Bayes**: P(H|E) = P(E|H)¬∑P(H) / P(E)
- **Na√Øve Bayes**: Clasificador probabil√≠stico que asume independencia

### L√≥gica Difusa:
- **Funci√≥n de pertenencia**: Valor entre 0 y 1
- **Variables ling√º√≠sticas**: Valores en lenguaje natural (bajo, medio, alto)
- **Modificadores**: "muy" (reduce), "m√°s o menos" (aumenta)

### Historia del PLN:
- **1940-1950**: Paradigmas fundacionales (aut√≥matas, teor√≠a de la informaci√≥n)
- **1957-1970**: Simb√≥lico vs Estoc√°stico
- **1970-1983**: 4 paradigmas (estoc√°stico, l√≥gica, comprensi√≥n, discurso)
- **1983-1993**: Revivir del empirismo
- **1994-1999**: Uni√≥n de vertientes
- **2000-presente**: Auge del Machine Learning y Deep Learning

### Hitos Importantes PLN:
- **1952**: Primer sistema de reconocimiento de voz (Bell Labs)
- **1963**: Brown Corpus (primer corpus online)
- **1972**: SHRDLU (Winograd)
- **2006**: T√©rmino "Deep Learning" (Hinton)

### Conocimientos del Lenguaje:
1. Fon√©tica/Fonolog√≠a (sonidos)
2. Morfolog√≠a (componentes de palabras)
3. Sintaxis (estructura)
4. Sem√°ntica (significado)
5. Pragm√°tica (contexto)
6. Discurso (texto completo)

---

## üí° CONSEJOS DE ESTUDIO

1. **Entiende las 4 capacidades** de la automatizaci√≥n inteligente y c√≥mo se relacionan
2. **Diferencia** entre sistemas basados en reglas, factores de certeza y l√≥gica difusa
3. **Conoce la cronolog√≠a** del PLN y los paradigmas de cada √©poca
4. **Identifica las tecnolog√≠as clave** de cada capacidad de IA
5. **Relaciona conceptos**: ¬øC√≥mo Deep Learning cambi√≥ el PLN?
6. **Comprende los 6 tipos de conocimiento** del lenguaje y su interrelaci√≥n

---

_Resumen creado para Semana 3 - Noviembre 2025_
